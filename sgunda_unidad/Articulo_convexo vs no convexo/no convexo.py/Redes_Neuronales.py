# Implementaci√≥n de Redes Neuronales para Dataset de C√°ncer de Mama Wisconsin
# Proyecto: T√©cnicas de Optimizaci√≥n Convexa y No Convexa

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix
import time
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# 1. CARGA Y EXPLORACI√ìN DEL DATASET
# =============================================================================

print("="*60)
print("IMPLEMENTACI√ìN DE REDES NEURONALES")
print("Dataset: Wisconsin Breast Cancer")
print("="*60)

# Cargar el dataset
data = load_breast_cancer()
X = data.data  # Caracter√≠sticas (30 features)
y = data.target  # Etiquetas (0=maligno, 1=benigno)

print(f"\nüìä INFORMACI√ìN DEL DATASET:")
print(f"‚Ä¢ N√∫mero de muestras: {X.shape[0]}")
print(f"‚Ä¢ N√∫mero de caracter√≠sticas: {X.shape[1]}")
print(f"‚Ä¢ Clases: {data.target_names}")
print(f"‚Ä¢ Distribuci√≥n de clases:")
unique, counts = np.unique(y, return_counts=True)
for i, (clase, count) in enumerate(zip(data.target_names, counts)):
    print(f"  - {clase}: {count} ({count/len(y)*100:.1f}%)")

print(f"\nüß† CONFIGURACI√ìN DE RED NEURONAL:")
print("‚Ä¢ Arquitectura: MultiLayer Perceptron (MLP)")
print("‚Ä¢ Capas de entrada: 30 neuronas (features)")
print("‚Ä¢ Capas ocultas: Variable (hiperpar√°metro)")
print("‚Ä¢ Capa de salida: 2 neuronas (clasificaci√≥n binaria)")
print("‚Ä¢ Funci√≥n de activaci√≥n: ReLU (capas ocultas), Softmax (salida)")

# =============================================================================
# 2. PREPROCESAMIENTO DE DATOS
# =============================================================================

print(f"\nüîß PREPROCESAMIENTO:")

# Divisi√≥n entrenamiento/prueba (80/20) - MISMA DIVISI√ìN QUE M√âTODOS ANTERIORES
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"‚Ä¢ Conjunto de entrenamiento: {X_train.shape[0]} muestras")
print(f"‚Ä¢ Conjunto de prueba: {X_test.shape[0]} muestras")

# Estandarizaci√≥n (CR√çTICA para redes neuronales)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"‚Ä¢ Estandarizaci√≥n aplicada ‚úì (CR√çTICA para redes neuronales)")
print(f"‚Ä¢ Media de caracter√≠sticas despu√©s de escalar: {np.mean(X_train_scaled, axis=0)[:3].round(3)}")
print(f"‚Ä¢ Desviaci√≥n est√°ndar despu√©s de escalar: {np.std(X_train_scaled, axis=0)[:3].round(3)}")

# =============================================================================
# 3. OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS
# =============================================================================

print(f"\n‚öôÔ∏è OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS:")

# Definir grid de b√∫squeda para arquitectura y regularizaci√≥n
param_grid = {
    'hidden_layer_sizes': [
        (50,),          # 1 capa con 50 neuronas
        (100,),         # 1 capa con 100 neuronas
        (50, 25),       # 2 capas: 50 y 25 neuronas
        (100, 50),      # 2 capas: 100 y 50 neuronas
        (100, 50, 25)   # 3 capas: 100, 50 y 25 neuronas
    ],
    'alpha': [0.0001, 0.001, 0.01],  # Regularizaci√≥n L2
    'learning_rate_init': [0.001, 0.01]  # Tasa de aprendizaje inicial
}

print(f"‚Ä¢ Arquitecturas a evaluar: {len(param_grid['hidden_layer_sizes'])}")
print(f"‚Ä¢ Configuraciones totales: {len(param_grid['hidden_layer_sizes']) * len(param_grid['alpha']) * len(param_grid['learning_rate_init'])}")

# Grid Search con validaci√≥n cruzada
mlp_grid = GridSearchCV(
    MLPClassifier(
        max_iter=1000,
        random_state=42,
        early_stopping=True,
        validation_fraction=0.1,
        n_iter_no_change=10
    ),
    param_grid,
    cv=3,  # Reducido para redes neuronales (m√°s costoso)
    scoring='f1',
    n_jobs=-1,
    verbose=0
)

print("‚Ä¢ Ejecutando Grid Search con validaci√≥n cruzada 3-fold...")
print("‚Ä¢ (Reducido a 3-fold debido al costo computacional de redes neuronales)")
start_time = time.time()
mlp_grid.fit(X_train_scaled, y_train)
grid_time = time.time() - start_time

print(f"‚Ä¢ Mejores hiperpar√°metros: {mlp_grid.best_params_}")
print(f"‚Ä¢ Mejor puntuaci√≥n F1 (CV): {mlp_grid.best_score_:.4f}")
print(f"‚Ä¢ Tiempo de optimizaci√≥n: {grid_time:.2f} segundos")

# =============================================================================
# 4. ENTRENAMIENTO DEL MODELO FINAL
# =============================================================================

print(f"\nüöÄ ENTRENAMIENTO DEL MODELO FINAL:")

# Usar los mejores hiperpar√°metros
best_mlp = mlp_grid.best_estimator_

# Reentrenar con datos completos de entrenamiento
start_time = time.time()
best_mlp.fit(X_train_scaled, y_train)
training_time = time.time() - start_time

print(f"‚Ä¢ Red neuronal entrenada con hiperpar√°metros √≥ptimos")
print(f"‚Ä¢ Tiempo de convergencia: {training_time:.4f} segundos")
print(f"‚Ä¢ Arquitectura final: {best_mlp.hidden_layer_sizes}")
print(f"‚Ä¢ N√∫mero de iteraciones: {best_mlp.n_iter_}")
print(f"‚Ä¢ Funci√≥n de p√©rdida final: {best_mlp.loss_:.6f}")

# Informaci√≥n de la arquitectura
total_params = 0
layer_sizes = [X_train_scaled.shape[1]] + list(best_mlp.hidden_layer_sizes) + [len(np.unique(y_train))]
for i in range(len(layer_sizes)-1):
    params_layer = layer_sizes[i] * layer_sizes[i+1] + layer_sizes[i+1]  # pesos + sesgos
    total_params += params_layer
    print(f"‚Ä¢ Capa {i+1}: {layer_sizes[i]} ‚Üí {layer_sizes[i+1]} ({params_layer} par√°metros)")

print(f"‚Ä¢ Total de par√°metros en la red: {total_params}")

# =============================================================================
# 5. EVALUACI√ìN DEL MODELO
# =============================================================================

print(f"\nüìä EVALUACI√ìN DEL MODELO:")

# Predicciones
y_pred = best_mlp.predict(X_test_scaled)
y_pred_proba = best_mlp.predict_proba(X_test_scaled)[:, 1]

# Calcular m√©tricas
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc_roc = roc_auc_score(y_test, y_pred_proba)

# Mostrar resultados
print(f"\nüìà M√âTRICAS DE RENDIMIENTO:")
print(f"‚Ä¢ Precisi√≥n (Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"‚Ä¢ Precisi√≥n (Precision): {precision:.4f} ({precision*100:.2f}%)")
print(f"‚Ä¢ Sensibilidad (Recall): {recall:.4f} ({recall*100:.2f}%)")
print(f"‚Ä¢ Puntuaci√≥n F1: {f1:.4f} ({f1*100:.2f}%)")
print(f"‚Ä¢ AUC-ROC: {auc_roc:.4f} ({auc_roc*100:.2f}%)")
print(f"‚Ä¢ Tiempo de Convergencia: {training_time:.4f} segundos")

# =============================================================================
# 6. AN√ÅLISIS DETALLADO
# =============================================================================

print(f"\nüîç AN√ÅLISIS DETALLADO:")

# Matriz de confusi√≥n
cm = confusion_matrix(y_test, y_pred)
print(f"\nüìã MATRIZ DE CONFUSI√ìN:")
print(f"                Predicho")
print(f"              Maligno  Benigno")
print(f"Real Maligno     {cm[0,0]:3d}     {cm[0,1]:3d}")
print(f"     Benigno     {cm[1,0]:3d}     {cm[1,1]:3d}")

# Calcular falsos positivos y negativos
tn, fp, fn, tp = cm.ravel()
print(f"\nüìä DESGLOSE DE PREDICCIONES:")
print(f"‚Ä¢ Verdaderos Positivos (TP): {tp}")
print(f"‚Ä¢ Verdaderos Negativos (TN): {tn}")
print(f"‚Ä¢ Falsos Positivos (FP): {fp}")
print(f"‚Ä¢ Falsos Negativos (FN): {fn}")

# An√°lisis de la confianza de predicciones
print(f"\nüéØ AN√ÅLISIS DE CONFIANZA:")
confidence_malignant = y_pred_proba[y_test == 0]  # Probabilidades para casos malignos reales
confidence_benign = y_pred_proba[y_test == 1]    # Probabilidades para casos benignos reales

print(f"‚Ä¢ Confianza promedio en casos malignos: {1-confidence_malignant.mean():.3f}")
print(f"‚Ä¢ Confianza promedio en casos benignos: {confidence_benign.mean():.3f}")
print(f"‚Ä¢ Desviaci√≥n est√°ndar confianza malignos: {confidence_malignant.std():.3f}")
print(f"‚Ä¢ Desviaci√≥n est√°ndar confianza benignos: {confidence_benign.std():.3f}")

# Informaci√≥n sobre convergencia
print(f"\nüîÑ AN√ÅLISIS DE CONVERGENCIA:")
print(f"‚Ä¢ ¬øConvergi√≥?: {'S√≠' if best_mlp.n_iter_ < best_mlp.max_iter else 'No (m√°x iteraciones)'}")
print(f"‚Ä¢ Iteraciones usadas: {best_mlp.n_iter_} de {best_mlp.max_iter}")
print(f"‚Ä¢ P√©rdida final: {best_mlp.loss_:.6f}")

# =============================================================================
# 7. VALIDACI√ìN CRUZADA ADICIONAL
# =============================================================================

print(f"\n‚úÖ VALIDACI√ìN CRUZADA FINAL:")

# Validaci√≥n cruzada con m√∫ltiples m√©tricas (3-fold para redes neuronales)
cv_accuracy = cross_val_score(best_mlp, X_train_scaled, y_train, cv=3, scoring='accuracy')
cv_precision = cross_val_score(best_mlp, X_train_scaled, y_train, cv=3, scoring='precision')
cv_recall = cross_val_score(best_mlp, X_train_scaled, y_train, cv=3, scoring='recall')
cv_f1 = cross_val_score(best_mlp, X_train_scaled, y_train, cv=3, scoring='f1')

print(f"‚Ä¢ Accuracy CV (3-fold): {cv_accuracy.mean():.4f} ¬± {cv_accuracy.std():.4f}")
print(f"‚Ä¢ Precision CV (3-fold): {cv_precision.mean():.4f} ¬± {cv_precision.std():.4f}")
print(f"‚Ä¢ Recall CV (3-fold): {cv_recall.mean():.4f} ¬± {cv_recall.std():.4f}")
print(f"‚Ä¢ F1-Score CV (3-fold): {cv_f1.mean():.4f} ¬± {cv_f1.std():.4f}")

# =============================================================================
# 8. COMPARACI√ìN CON M√âTODOS CONVEXOS
# =============================================================================

print(f"\nüÜö COMPARACI√ìN CON M√âTODOS CONVEXOS:")
print("(Valores de referencia)")
print(f"‚Ä¢ Regresi√≥n Log√≠stica - Accuracy: 0.974")
print(f"‚Ä¢ SVM Lineal - Accuracy: 0.982")
print(f"‚Ä¢ Regresi√≥n Ridge - Accuracy: 0.956")
print(f"‚Ä¢ Redes Neuronales - Accuracy: {accuracy:.3f}")
print(f"‚Ä¢ Posici√≥n: {'üèÜ Mejor' if accuracy > 0.982 else 'ü•à Segundo' if accuracy > 0.974 else 'ü•â Tercero' if accuracy > 0.956 else '4to lugar'}")

# =============================================================================
# 9. RESUMEN FINAL PARA EL PAPER
# =============================================================================

print(f"\n" + "="*60)
print("RESUMEN PARA EL PAPER - REDES NEURONALES")
print("="*60)

print(f"\nüìä RESULTADOS FINALES:")
print(f"‚Ä¢ Arquitectura √≥ptima: {best_mlp.hidden_layer_sizes}")
print(f"‚Ä¢ Hiperpar√°metros √≥ptimos:")
print(f"  - Alpha (regularizaci√≥n): {best_mlp.alpha}")
print(f"  - Learning rate: {best_mlp.learning_rate_init}")
print(f"‚Ä¢ Precisi√≥n (Accuracy): {accuracy:.3f}")
print(f"‚Ä¢ Precisi√≥n (Precision): {precision:.3f}")
print(f"‚Ä¢ Sensibilidad (Recall): {recall:.3f}")
print(f"‚Ä¢ Puntuaci√≥n F1: {f1:.3f}")
print(f"‚Ä¢ AUC-ROC: {auc_roc:.3f}")
print(f"‚Ä¢ Tiempo de Convergencia: {training_time:.4f}s")
print(f"‚Ä¢ Total de par√°metros: {total_params}")
print(f"‚Ä¢ Iteraciones de convergencia: {best_mlp.n_iter_}")

print(f"\nüéØ INTERPRETACI√ìN CL√çNICA:")
if recall >= 0.95:
    print("‚Ä¢ Excelente detecci√≥n de casos malignos (recall alto)")
elif recall >= 0.90:
    print("‚Ä¢ Buena detecci√≥n de casos malignos")
else:
    print("‚Ä¢ Detecci√≥n moderada de casos malignos")

if precision >= 0.95:
    print("‚Ä¢ Muy pocos falsos positivos (precision alta)")
elif precision >= 0.90:
    print("‚Ä¢ Pocos falsos positivos")
else:
    print("‚Ä¢ Algunos falsos positivos presentes")

print(f"\n‚ú® VENTAJAS DE REDES NEURONALES:")
print("‚Ä¢ Capacidad de modelar relaciones no lineales complejas")
print("‚Ä¢ Aproximador universal de funciones")
print("‚Ä¢ Flexibilidad en arquitectura")
print("‚Ä¢ Probabilidades de salida bien calibradas")

print(f"\n‚ö†Ô∏è CONSIDERACIONES NO CONVEXAS:")
print("‚Ä¢ M√∫ltiples √≥ptimos locales posibles")
print("‚Ä¢ Sensible a inicializaci√≥n de pesos")
print("‚Ä¢ Requiere m√°s tiempo de entrenamiento")
print("‚Ä¢ Mayor riesgo de sobreajuste")

print(f"\nüîß COMPLEJIDAD COMPUTACIONAL:")
complexity_level = "Baja" if total_params < 1000 else "Media" if total_params < 5000 else "Alta"
print(f"‚Ä¢ Complejidad del modelo: {complexity_level} ({total_params} par√°metros)")
print(f"‚Ä¢ Tiempo relativo: {'R√°pido' if training_time < 0.1 else 'Moderado' if training_time < 1.0 else 'Lento'}")

print(f"\n" + "="*60)
print("¬°Implementaci√≥n de Redes Neuronales completada!")
print("="*60)