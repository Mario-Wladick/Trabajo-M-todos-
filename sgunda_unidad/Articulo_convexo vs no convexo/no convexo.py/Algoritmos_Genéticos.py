# Implementaci√≥n de Algoritmos Gen√©ticos para Dataset de C√°ncer de Mama Wisconsin
# Proyecto: T√©cnicas de Optimizaci√≥n Convexa y No Convexa

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix
import time
import random
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# 1. CARGA Y EXPLORACI√ìN DEL DATASET
# =============================================================================

print("="*60)
print("IMPLEMENTACI√ìN DE ALGORITMOS GEN√âTICOS")
print("Dataset: Wisconsin Breast Cancer")
print("="*60)

# Cargar el dataset
data = load_breast_cancer()
X = data.data  # Caracter√≠sticas (30 features)
y = data.target  # Etiquetas (0=maligno, 1=benigno)

print(f"\nüìä INFORMACI√ìN DEL DATASET:")
print(f"‚Ä¢ N√∫mero de muestras: {X.shape[0]}")
print(f"‚Ä¢ N√∫mero de caracter√≠sticas: {X.shape[1]}")
print(f"‚Ä¢ Clases: {data.target_names}")
print(f"‚Ä¢ Distribuci√≥n de clases:")
unique, counts = np.unique(y, return_counts=True)
for i, (clase, count) in enumerate(zip(data.target_names, counts)):
    print(f"  - {clase}: {count} ({count/len(y)*100:.1f}%)")

print(f"\nüß¨ CONFIGURACI√ìN DE ALGORITMO GEN√âTICO:")
print("‚Ä¢ Problema: Selecci√≥n de caracter√≠sticas + optimizaci√≥n de hiperpar√°metros")
print("‚Ä¢ Cromosoma: [feature_mask(30 bits)] + [C_exp(4 bits)] + [solver(2 bits)]")
print("‚Ä¢ Longitud total: 36 bits por individuo")
print("‚Ä¢ Funci√≥n objetivo: F1-Score con validaci√≥n cruzada")
print("‚Ä¢ Operadores: Selecci√≥n por torneo, cruce uniforme, mutaci√≥n bit-flip")

# =============================================================================
# 2. PREPROCESAMIENTO DE DATOS
# =============================================================================

print(f"\nüîß PREPROCESAMIENTO:")

# Divisi√≥n entrenamiento/prueba (80/20) - MISMA DIVISI√ìN QUE M√âTODOS ANTERIORES
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"‚Ä¢ Conjunto de entrenamiento: {X_train.shape[0]} muestras")
print(f"‚Ä¢ Conjunto de prueba: {X_test.shape[0]} muestras")

# Estandarizaci√≥n
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"‚Ä¢ Estandarizaci√≥n aplicada ‚úì")

# =============================================================================
# 3. IMPLEMENTACI√ìN DEL ALGORITMO GEN√âTICO
# =============================================================================

class GeneticAlgorithmOptimizer:
    def __init__(self, X_train, y_train, population_size=50, generations=30, 
                 mutation_rate=0.1, crossover_rate=0.8, tournament_size=5):
        self.X_train = X_train
        self.y_train = y_train
        self.population_size = population_size
        self.generations = generations
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        self.tournament_size = tournament_size
        self.n_features = X_train.shape[1]
        
        # Mapeos para hiperpar√°metros
        self.C_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]  # 2^3 = 8 valores
        self.solvers = ['liblinear', 'lbfgs', 'newton-cg', 'sag']  # 2^2 = 4 valores
        
        # Historial de evoluci√≥n
        self.fitness_history = []
        self.best_fitness_history = []
        
    def create_individual(self):
        """Crear un individuo aleatorio"""
        # 30 bits para features + 3 bits para C + 2 bits para solver
        chromosome = np.random.randint(0, 2, self.n_features + 3 + 2)
        return chromosome
    
    def decode_chromosome(self, chromosome):
        """Decodificar cromosoma a par√°metros"""
        # Selecci√≥n de caracter√≠sticas (primeros 30 bits)
        feature_mask = chromosome[:self.n_features].astype(bool)
        
        # Si no hay caracter√≠sticas seleccionadas, seleccionar al menos una
        if not np.any(feature_mask):
            feature_mask[np.random.randint(0, self.n_features)] = True
            
        # Par√°metro C (siguientes 3 bits)
        c_bits = chromosome[self.n_features:self.n_features+3]
        c_index = int(''.join(map(str, c_bits)), 2) % len(self.C_values)
        C = self.C_values[c_index]
        
        # Solver (√∫ltimos 2 bits)
        solver_bits = chromosome[self.n_features+3:self.n_features+5]
        solver_index = int(''.join(map(str, solver_bits)), 2) % len(self.solvers)
        solver = self.solvers[solver_index]
        
        return feature_mask, C, solver
    
    def evaluate_fitness(self, chromosome):
        """Evaluar fitness de un individuo"""
        try:
            feature_mask, C, solver = self.decode_chromosome(chromosome)
            
            # Seleccionar caracter√≠sticas
            X_selected = self.X_train[:, feature_mask]
            
            # Crear y evaluar modelo
            model = LogisticRegression(C=C, solver=solver, random_state=42, max_iter=1000)
            
            # Validaci√≥n cruzada 3-fold (reducida para velocidad)
            scores = cross_val_score(model, X_selected, self.y_train, cv=3, scoring='f1')
            fitness = scores.mean()
            
            # Penalizar por usar demasiadas caracter√≠sticas (parsimonia)
            n_features_used = np.sum(feature_mask)
            parsimony_penalty = 0.001 * n_features_used / self.n_features
            fitness = fitness - parsimony_penalty
            
            return fitness
        except:
            return 0.0  # Fitness muy bajo para configuraciones inv√°lidas
    
    def tournament_selection(self, population, fitness_scores):
        """Selecci√≥n por torneo"""
        selected = []
        for _ in range(len(population)):
            tournament_indices = np.random.choice(len(population), self.tournament_size, replace=False)
            tournament_fitness = [fitness_scores[i] for i in tournament_indices]
            winner_index = tournament_indices[np.argmax(tournament_fitness)]
            selected.append(population[winner_index].copy())
        return selected
    
    def uniform_crossover(self, parent1, parent2):
        """Cruce uniforme"""
        if np.random.random() > self.crossover_rate:
            return parent1.copy(), parent2.copy()
        
        child1 = parent1.copy()
        child2 = parent2.copy()
        
        for i in range(len(parent1)):
            if np.random.random() < 0.5:
                child1[i], child2[i] = child2[i], child1[i]
                
        return child1, child2
    
    def mutate(self, individual):
        """Mutaci√≥n bit-flip"""
        mutated = individual.copy()
        for i in range(len(individual)):
            if np.random.random() < self.mutation_rate:
                mutated[i] = 1 - mutated[i]  # Flip bit
        return mutated
    
    def evolve(self):
        """Algoritmo gen√©tico principal"""
        print(f"\nüß¨ INICIANDO EVOLUCI√ìN:")
        print(f"‚Ä¢ Poblaci√≥n: {self.population_size} individuos")
        print(f"‚Ä¢ Generaciones: {self.generations}")
        print(f"‚Ä¢ Tasa de mutaci√≥n: {self.mutation_rate}")
        print(f"‚Ä¢ Tasa de cruce: {self.crossover_rate}")
        
        # Inicializar poblaci√≥n
        population = [self.create_individual() for _ in range(self.population_size)]
        
        start_time = time.time()
        
        for generation in range(self.generations):
            # Evaluar fitness
            fitness_scores = [self.evaluate_fitness(ind) for ind in population]
            
            # Estad√≠sticas de la generaci√≥n
            avg_fitness = np.mean(fitness_scores)
            best_fitness = np.max(fitness_scores)
            
            self.fitness_history.append(avg_fitness)
            self.best_fitness_history.append(best_fitness)
            
            if generation % 5 == 0:
                print(f"‚Ä¢ Generaci√≥n {generation:2d}: Fitness promedio = {avg_fitness:.4f}, Mejor = {best_fitness:.4f}")
            
            # Selecci√≥n
            selected = self.tournament_selection(population, fitness_scores)
            
            # Cruce y mutaci√≥n
            new_population = []
            for i in range(0, len(selected), 2):
                parent1 = selected[i]
                parent2 = selected[(i + 1) % len(selected)]
                
                child1, child2 = self.uniform_crossover(parent1, parent2)
                child1 = self.mutate(child1)
                child2 = self.mutate(child2)
                
                new_population.extend([child1, child2])
            
            population = new_population[:self.population_size]
        
        evolution_time = time.time() - start_time
        
        # Encontrar mejor individuo final
        final_fitness_scores = [self.evaluate_fitness(ind) for ind in population]
        best_index = np.argmax(final_fitness_scores)
        best_individual = population[best_index]
        best_fitness = final_fitness_scores[best_index]
        
        print(f"‚Ä¢ Evoluci√≥n completada en {evolution_time:.2f} segundos")
        print(f"‚Ä¢ Mejor fitness final: {best_fitness:.4f}")
        
        return best_individual, best_fitness, evolution_time

# =============================================================================
# 4. EJECUCI√ìN DEL ALGORITMO GEN√âTICO
# =============================================================================

print(f"\n‚öôÔ∏è OPTIMIZACI√ìN CON ALGORITMO GEN√âTICO:")

# Crear optimizador gen√©tico
ga_optimizer = GeneticAlgorithmOptimizer(
    X_train_scaled, y_train,
    population_size=50,
    generations=30,
    mutation_rate=0.1,
    crossover_rate=0.8,
    tournament_size=5
)

# Ejecutar evoluci√≥n
best_chromosome, best_fitness_cv, evolution_time = ga_optimizer.evolve()

# Decodificar mejor soluci√≥n
best_features, best_C, best_solver = ga_optimizer.decode_chromosome(best_chromosome)

print(f"\nüèÜ MEJOR SOLUCI√ìN ENCONTRADA:")
print(f"‚Ä¢ Caracter√≠sticas seleccionadas: {np.sum(best_features)} de {len(best_features)}")
print(f"‚Ä¢ C √≥ptimo: {best_C}")
print(f"‚Ä¢ Solver √≥ptimo: {best_solver}")
print(f"‚Ä¢ Fitness (F1-Score CV): {best_fitness_cv:.4f}")

# =============================================================================
# 5. ENTRENAMIENTO DEL MODELO FINAL
# =============================================================================

print(f"\nüöÄ ENTRENAMIENTO DEL MODELO FINAL:")

# Crear modelo con mejores par√°metros
best_model = LogisticRegression(C=best_C, solver=best_solver, random_state=42, max_iter=1000)

# Entrenar con caracter√≠sticas seleccionadas
X_train_selected = X_train_scaled[:, best_features]
X_test_selected = X_test_scaled[:, best_features]

start_time = time.time()
best_model.fit(X_train_selected, y_train)
training_time = time.time() - start_time

print(f"‚Ä¢ Modelo entrenado con caracter√≠sticas seleccionadas")
print(f"‚Ä¢ Tiempo de entrenamiento final: {training_time:.4f} segundos")
print(f"‚Ä¢ Tiempo total (evoluci√≥n + entrenamiento): {evolution_time + training_time:.2f} segundos")

# =============================================================================
# 6. EVALUACI√ìN DEL MODELO
# =============================================================================

print(f"\nüìä EVALUACI√ìN DEL MODELO:")

# Predicciones
y_pred = best_model.predict(X_test_selected)
y_pred_proba = best_model.predict_proba(X_test_selected)[:, 1]

# Calcular m√©tricas
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc_roc = roc_auc_score(y_test, y_pred_proba)

# Mostrar resultados
print(f"\nüìà M√âTRICAS DE RENDIMIENTO:")
print(f"‚Ä¢ Precisi√≥n (Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"‚Ä¢ Precisi√≥n (Precision): {precision:.4f} ({precision*100:.2f}%)")
print(f"‚Ä¢ Sensibilidad (Recall): {recall:.4f} ({recall*100:.2f}%)")
print(f"‚Ä¢ Puntuaci√≥n F1: {f1:.4f} ({f1*100:.2f}%)")
print(f"‚Ä¢ AUC-ROC: {auc_roc:.4f} ({auc_roc*100:.2f}%)")
print(f"‚Ä¢ Tiempo Total: {evolution_time + training_time:.4f} segundos")

# =============================================================================
# 7. AN√ÅLISIS DETALLADO
# =============================================================================

print(f"\nüîç AN√ÅLISIS DETALLADO:")

# Matriz de confusi√≥n
cm = confusion_matrix(y_test, y_pred)
print(f"\nüìã MATRIZ DE CONFUSI√ìN:")
print(f"                Predicho")
print(f"              Maligno  Benigno")
print(f"Real Maligno     {cm[0,0]:3d}     {cm[0,1]:3d}")
print(f"     Benigno     {cm[1,0]:3d}     {cm[1,1]:3d}")

# Calcular falsos positivos y negativos
tn, fp, fn, tp = cm.ravel()
print(f"\nüìä DESGLOSE DE PREDICCIONES:")
print(f"‚Ä¢ Verdaderos Positivos (TP): {tp}")
print(f"‚Ä¢ Verdaderos Negativos (TN): {tn}")
print(f"‚Ä¢ Falsos Positivos (FP): {fp}")
print(f"‚Ä¢ Falsos Negativos (FN): {fn}")

# An√°lisis de caracter√≠sticas seleccionadas
print(f"\nüß¨ AN√ÅLISIS DE EVOLUCI√ìN:")
print(f"‚Ä¢ Caracter√≠sticas seleccionadas: {np.sum(best_features)} de {data.feature_names.shape[0]}")
print(f"‚Ä¢ Reducci√≥n dimensional: {(1 - np.sum(best_features)/len(best_features))*100:.1f}%")

selected_features = data.feature_names[best_features]
print(f"\nüîù CARACTER√çSTICAS SELECCIONADAS POR ALGORITMO GEN√âTICO:")
for i, feature in enumerate(selected_features[:10]):  # Mostrar m√°ximo 10
    print(f"  {i+1:2d}. {feature}")
if len(selected_features) > 10:
    print(f"  ... y {len(selected_features)-10} m√°s")

# An√°lisis de convergencia
print(f"\nüìà AN√ÅLISIS DE CONVERGENCIA:")
print(f"‚Ä¢ Fitness inicial promedio: {ga_optimizer.fitness_history[0]:.4f}")
print(f"‚Ä¢ Fitness final promedio: {ga_optimizer.fitness_history[-1]:.4f}")
print(f"‚Ä¢ Mejora total: {ga_optimizer.fitness_history[-1] - ga_optimizer.fitness_history[0]:+.4f}")
print(f"‚Ä¢ Mejor fitness en evoluci√≥n: {max(ga_optimizer.best_fitness_history):.4f}")

# =============================================================================
# 8. COMPARACI√ìN CON M√âTODOS ANTERIORES
# =============================================================================

print(f"\nüÜö COMPARACI√ìN CON M√âTODOS ANTERIORES:")
print("(Valores de referencia)")
print(f"‚Ä¢ SVM Lineal - Accuracy: 0.982")
print(f"‚Ä¢ SVM RBF - Accuracy: 0.982")
print(f"‚Ä¢ Regresi√≥n Log√≠stica - Accuracy: 0.974")
print(f"‚Ä¢ Redes Neuronales - Accuracy: 0.956")
print(f"‚Ä¢ Regresi√≥n Ridge - Accuracy: 0.956")
print(f"‚Ä¢ Algoritmos Gen√©ticos - Accuracy: {accuracy:.3f}")

ranking_methods = [
    ("SVM Lineal", 0.982), ("SVM RBF", 0.982), ("Regresi√≥n Log√≠stica", 0.974),
    ("Redes Neuronales", 0.956), ("Regresi√≥n Ridge", 0.956), ("Algoritmos Gen√©ticos", accuracy)
]
ranking_methods.sort(key=lambda x: x[1], reverse=True)

print(f"\nüèÜ RANKING FINAL:")
for i, (method, acc) in enumerate(ranking_methods):
    emoji = "ü•á" if i == 0 else "ü•à" if i == 1 else "ü•â" if i == 2 else f"{i+1}¬∞"
    print(f"  {emoji} {method}: {acc:.3f}")

# =============================================================================
# 9. RESUMEN FINAL PARA EL PAPER
# =============================================================================

print(f"\n" + "="*60)
print("RESUMEN PARA EL PAPER - ALGORITMOS GEN√âTICOS")
print("="*60)

print(f"\nüìä RESULTADOS FINALES:")
print(f"‚Ä¢ Par√°metros evolutivos:")
print(f"  - Poblaci√≥n: {ga_optimizer.population_size} individuos")
print(f"  - Generaciones: {ga_optimizer.generations}")
print(f"  - Tasa de mutaci√≥n: {ga_optimizer.mutation_rate}")
print(f"  - Tasa de cruce: {ga_optimizer.crossover_rate}")
print(f"‚Ä¢ Soluci√≥n √≥ptima:")
print(f"  - Caracter√≠sticas: {np.sum(best_features)} de {len(best_features)} ({np.sum(best_features)/len(best_features)*100:.1f}%)")
print(f"  - C: {best_C}")
print(f"  - Solver: {best_solver}")
print(f"‚Ä¢ Precisi√≥n (Accuracy): {accuracy:.3f}")
print(f"‚Ä¢ Precisi√≥n (Precision): {precision:.3f}")
print(f"‚Ä¢ Sensibilidad (Recall): {recall:.3f}")
print(f"‚Ä¢ Puntuaci√≥n F1: {f1:.3f}")
print(f"‚Ä¢ AUC-ROC: {auc_roc:.3f}")
print(f"‚Ä¢ Tiempo Total: {evolution_time + training_time:.4f}s")

print(f"\nüéØ INTERPRETACI√ìN CL√çNICA:")
if recall >= 0.95:
    print("‚Ä¢ Excelente detecci√≥n de casos malignos (recall alto)")
elif recall >= 0.90:
    print("‚Ä¢ Buena detecci√≥n de casos malignos")
else:
    print("‚Ä¢ Detecci√≥n moderada de casos malignos")

if precision >= 0.95:
    print("‚Ä¢ Muy pocos falsos positivos (precision alta)")
elif precision >= 0.90:
    print("‚Ä¢ Pocos falsos positivos")
else:
    print("‚Ä¢ Algunos falsos positivos presentes")

print(f"\n‚ú® VENTAJAS DE ALGORITMOS GEN√âTICOS:")
print("‚Ä¢ Optimizaci√≥n simult√°nea de selecci√≥n de caracter√≠sticas e hiperpar√°metros")
print("‚Ä¢ No requiere gradientes - optimizaci√≥n libre de derivadas")
print("‚Ä¢ Exploraci√≥n global del espacio de b√∫squeda")
print("‚Ä¢ Capacidad de escape de √≥ptimos locales")

print(f"\n‚ö†Ô∏è CONSIDERACIONES NO CONVEXAS:")
print("‚Ä¢ Optimizaci√≥n estoc√°stica - resultados variables entre ejecuciones")
print("‚Ä¢ Mayor tiempo computacional debido a evaluaciones iterativas")
print("‚Ä¢ Convergencia no garantizada al √≥ptimo global")
print("‚Ä¢ Sensible a par√°metros evolutivos (poblaci√≥n, generaciones, etc.)")

print(f"\nüî¨ ASPECTOS EVOLUTIVOS:")
print(f"‚Ä¢ Selecci√≥n autom√°tica de caracter√≠sticas: {(1-np.sum(best_features)/len(best_features))*100:.1f}% reducci√≥n")
print(f"‚Ä¢ Mejora evolutiva: {max(ga_optimizer.best_fitness_history) - ga_optimizer.fitness_history[0]:+.4f} en F1-Score")
print(f"‚Ä¢ Eficiencia temporal: {evolution_time/(ga_optimizer.population_size * ga_optimizer.generations)*1000:.2f}ms por evaluaci√≥n")

print(f"\n" + "="*60)
print("¬°Implementaci√≥n de Algoritmos Gen√©ticos completada!")
print("="*60)