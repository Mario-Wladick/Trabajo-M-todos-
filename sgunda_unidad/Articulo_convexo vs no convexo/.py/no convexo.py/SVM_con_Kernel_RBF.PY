# ImplementaciÃ³n de SVM con Kernel RBF para Dataset de CÃ¡ncer de Mama Wisconsin
# Proyecto: TÃ©cnicas de OptimizaciÃ³n Convexa y No Convexa

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix
import time
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# 1. CARGA Y EXPLORACIÃ“N DEL DATASET
# =============================================================================

print("="*60)
print("IMPLEMENTACIÃ“N DE SVM CON KERNEL RBF")
print("Dataset: Wisconsin Breast Cancer")
print("="*60)

# Cargar el dataset
data = load_breast_cancer()
X = data.data  # CaracterÃ­sticas (30 features)
y = data.target  # Etiquetas (0=maligno, 1=benigno)

print(f"\nğŸ“Š INFORMACIÃ“N DEL DATASET:")
print(f"â€¢ NÃºmero de muestras: {X.shape[0]}")
print(f"â€¢ NÃºmero de caracterÃ­sticas: {X.shape[1]}")
print(f"â€¢ Clases: {data.target_names}")
print(f"â€¢ DistribuciÃ³n de clases:")
unique, counts = np.unique(y, return_counts=True)
for i, (clase, count) in enumerate(zip(data.target_names, counts)):
    print(f"  - {clase}: {count} ({count/len(y)*100:.1f}%)")

print(f"\nğŸ”® CONFIGURACIÃ“N DE SVM RBF:")
print("â€¢ Kernel: Radial Basis Function (RBF)")
print("â€¢ FunciÃ³n del kernel: K(x,z) = exp(-Î³||x-z||Â²)")
print("â€¢ Mapeo: Espacio de caracterÃ­sticas de dimensiÃ³n infinita")
print("â€¢ Frontera de decisiÃ³n: No lineal en espacio original")
print("â€¢ OptimizaciÃ³n: No convexa debido al kernel no lineal")

# =============================================================================
# 2. PREPROCESAMIENTO DE DATOS
# =============================================================================

print(f"\nğŸ”§ PREPROCESAMIENTO:")

# DivisiÃ³n entrenamiento/prueba (80/20) - MISMA DIVISIÃ“N QUE MÃ‰TODOS ANTERIORES
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"â€¢ Conjunto de entrenamiento: {X_train.shape[0]} muestras")
print(f"â€¢ Conjunto de prueba: {X_test.shape[0]} muestras")

# EstandarizaciÃ³n (CRÃTICA para SVM RBF)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"â€¢ EstandarizaciÃ³n aplicada âœ“ (CRÃTICA para kernel RBF)")
print(f"â€¢ Media de caracterÃ­sticas despuÃ©s de escalar: {np.mean(X_train_scaled, axis=0)[:3].round(3)}")
print(f"â€¢ DesviaciÃ³n estÃ¡ndar despuÃ©s de escalar: {np.std(X_train_scaled, axis=0)[:3].round(3)}")

# =============================================================================
# 3. OPTIMIZACIÃ“N DE HIPERPARÃMETROS
# =============================================================================

print(f"\nâš™ï¸ OPTIMIZACIÃ“N DE HIPERPARÃMETROS:")

# Definir grid de bÃºsqueda para C y gamma (parÃ¡metros crÃ­ticos del RBF)
param_grid = {
    'C': [0.1, 1.0, 10.0, 100.0, 1000.0],        # ParÃ¡metro de regularizaciÃ³n
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1.0, 10.0]  # ParÃ¡metro del kernel RBF
}

print(f"â€¢ Valores de C a evaluar: {param_grid['C']}")
print(f"â€¢ Valores de gamma a evaluar: {param_grid['gamma']}")
print(f"â€¢ Configuraciones totales: {len(param_grid['C']) * len(param_grid['gamma'])}")

# Grid Search con validaciÃ³n cruzada
svm_rbf_grid = GridSearchCV(
    SVC(kernel='rbf', random_state=42, probability=True),
    param_grid,
    cv=5,  # 5-fold para mejor evaluaciÃ³n
    scoring='f1',
    n_jobs=-1,
    verbose=0
)

print("â€¢ Ejecutando Grid Search con validaciÃ³n cruzada 5-fold...")
print("â€¢ (Evaluando combinaciones C-gamma para optimizaciÃ³n no convexa)")
start_time = time.time()
svm_rbf_grid.fit(X_train_scaled, y_train)
grid_time = time.time() - start_time

print(f"â€¢ Mejores hiperparÃ¡metros: {svm_rbf_grid.best_params_}")
print(f"â€¢ Mejor puntuaciÃ³n F1 (CV): {svm_rbf_grid.best_score_:.4f}")
print(f"â€¢ Tiempo de optimizaciÃ³n: {grid_time:.2f} segundos")

# AnÃ¡lisis del espacio de hiperparÃ¡metros
print(f"\nğŸ¯ ANÃLISIS DEL ESPACIO DE HIPERPARÃMETROS:")
print(f"â€¢ C Ã³ptimo: {svm_rbf_grid.best_params_['C']}")
print(f"â€¢ Gamma Ã³ptimo: {svm_rbf_grid.best_params_['gamma']}")

if svm_rbf_grid.best_params_['gamma'] in ['scale', 'auto']:
    if svm_rbf_grid.best_params_['gamma'] == 'scale':
        gamma_value = 1 / (X_train_scaled.shape[1] * X_train_scaled.var())
        print(f"â€¢ Gamma 'scale' equivale a: {gamma_value:.6f}")
    else:
        gamma_value = 1 / X_train_scaled.shape[1]
        print(f"â€¢ Gamma 'auto' equivale a: {gamma_value:.6f}")
else:
    gamma_value = svm_rbf_grid.best_params_['gamma']
    print(f"â€¢ Gamma numÃ©rico: {gamma_value}")

# =============================================================================
# 4. ENTRENAMIENTO DEL MODELO FINAL
# =============================================================================

print(f"\nğŸš€ ENTRENAMIENTO DEL MODELO FINAL:")

# Usar los mejores hiperparÃ¡metros
best_svm_rbf = svm_rbf_grid.best_estimator_

# Medir tiempo de convergencia
start_time = time.time()
best_svm_rbf.fit(X_train_scaled, y_train)
training_time = time.time() - start_time

print(f"â€¢ Modelo SVM RBF entrenado con hiperparÃ¡metros Ã³ptimos")
print(f"â€¢ Tiempo de convergencia: {training_time:.4f} segundos")
print(f"â€¢ NÃºmero de vectores de soporte: {best_svm_rbf.n_support_}")
print(f"â€¢ Total de vectores de soporte: {np.sum(best_svm_rbf.n_support_)} de {len(X_train)} muestras ({np.sum(best_svm_rbf.n_support_)/len(X_train)*100:.1f}%)")

# Comparar complejidad con SVM lineal
print(f"â€¢ ComparaciÃ³n con SVM Lineal:")
print(f"  - SVM Lineal: 51 vectores de soporte (11.2%)")
print(f"  - SVM RBF: {np.sum(best_svm_rbf.n_support_)} vectores de soporte ({np.sum(best_svm_rbf.n_support_)/len(X_train)*100:.1f}%)")

# =============================================================================
# 5. EVALUACIÃ“N DEL MODELO
# =============================================================================

print(f"\nğŸ“Š EVALUACIÃ“N DEL MODELO:")

# Predicciones
y_pred = best_svm_rbf.predict(X_test_scaled)
y_pred_proba = best_svm_rbf.predict_proba(X_test_scaled)[:, 1]

# Calcular mÃ©tricas
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc_roc = roc_auc_score(y_test, y_pred_proba)

# Mostrar resultados
print(f"\nğŸ“ˆ MÃ‰TRICAS DE RENDIMIENTO:")
print(f"â€¢ PrecisiÃ³n (Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"â€¢ PrecisiÃ³n (Precision): {precision:.4f} ({precision*100:.2f}%)")
print(f"â€¢ Sensibilidad (Recall): {recall:.4f} ({recall*100:.2f}%)")
print(f"â€¢ PuntuaciÃ³n F1: {f1:.4f} ({f1*100:.2f}%)")
print(f"â€¢ AUC-ROC: {auc_roc:.4f} ({auc_roc*100:.2f}%)")
print(f"â€¢ Tiempo de Convergencia: {training_time:.4f} segundos")

# =============================================================================
# 6. ANÃLISIS DETALLADO
# =============================================================================

print(f"\nğŸ” ANÃLISIS DETALLADO:")

# Matriz de confusiÃ³n
cm = confusion_matrix(y_test, y_pred)
print(f"\nğŸ“‹ MATRIZ DE CONFUSIÃ“N:")
print(f"                Predicho")
print(f"              Maligno  Benigno")
print(f"Real Maligno     {cm[0,0]:3d}     {cm[0,1]:3d}")
print(f"     Benigno     {cm[1,0]:3d}     {cm[1,1]:3d}")

# Calcular falsos positivos y negativos
tn, fp, fn, tp = cm.ravel()
print(f"\nğŸ“Š DESGLOSE DE PREDICCIONES:")
print(f"â€¢ Verdaderos Positivos (TP): {tp}")
print(f"â€¢ Verdaderos Negativos (TN): {tn}")
print(f"â€¢ Falsos Positivos (FP): {fp}")
print(f"â€¢ Falsos Negativos (FN): {fn}")

# AnÃ¡lisis de vectores de soporte
print(f"\nğŸ¯ ANÃLISIS DE VECTORES DE SOPORTE:")
print(f"â€¢ Vectores de soporte por clase: {best_svm_rbf.n_support_}")
print(f"â€¢ Porcentaje total: {np.sum(best_svm_rbf.n_support_)/len(X_train)*100:.1f}%")

if np.sum(best_svm_rbf.n_support_)/len(X_train) > 0.3:
    print("â€¢ InterpretaciÃ³n: Frontera de decisiÃ³n compleja (muchos vectores necesarios)")
elif np.sum(best_svm_rbf.n_support_)/len(X_train) > 0.15:
    print("â€¢ InterpretaciÃ³n: Frontera de decisiÃ³n moderadamente compleja")
else:
    print("â€¢ InterpretaciÃ³n: Frontera de decisiÃ³n relativamente simple")

# AnÃ¡lisis de la funciÃ³n del kernel
print(f"\nğŸ”® ANÃLISIS DEL KERNEL RBF:")
print(f"â€¢ ParÃ¡metro gamma: {svm_rbf_grid.best_params_['gamma']}")
if isinstance(svm_rbf_grid.best_params_['gamma'], str):
    gamma_val = gamma_value
else:
    gamma_val = svm_rbf_grid.best_params_['gamma']

if gamma_val > 1:
    print("â€¢ Comportamiento: Kernel muy localizado (alta varianza, bajo sesgo)")
elif gamma_val > 0.1:
    print("â€¢ Comportamiento: Kernel moderadamente localizado")
else:
    print("â€¢ Comportamiento: Kernel suave (baja varianza, alto sesgo)")

# =============================================================================
# 7. VALIDACIÃ“N CRUZADA ADICIONAL
# =============================================================================

print(f"\nâœ… VALIDACIÃ“N CRUZADA FINAL:")

# ValidaciÃ³n cruzada con mÃºltiples mÃ©tricas
cv_accuracy = cross_val_score(best_svm_rbf, X_train_scaled, y_train, cv=5, scoring='accuracy')
cv_precision = cross_val_score(best_svm_rbf, X_train_scaled, y_train, cv=5, scoring='precision')
cv_recall = cross_val_score(best_svm_rbf, X_train_scaled, y_train, cv=5, scoring='recall')
cv_f1 = cross_val_score(best_svm_rbf, X_train_scaled, y_train, cv=5, scoring='f1')

print(f"â€¢ Accuracy CV (5-fold): {cv_accuracy.mean():.4f} Â± {cv_accuracy.std():.4f}")
print(f"â€¢ Precision CV (5-fold): {cv_precision.mean():.4f} Â± {cv_precision.std():.4f}")
print(f"â€¢ Recall CV (5-fold): {cv_recall.mean():.4f} Â± {cv_recall.std():.4f}")
print(f"â€¢ F1-Score CV (5-fold): {cv_f1.mean():.4f} Â± {cv_f1.std():.4f}")

# =============================================================================
# 8. COMPARACIÃ“N CON MÃ‰TODOS ANTERIORES
# =============================================================================

print(f"\nğŸ†š COMPARACIÃ“N CON MÃ‰TODOS ANTERIORES:")
print("(Valores de referencia)")
print(f"â€¢ SVM Lineal - Accuracy: 0.982")
print(f"â€¢ RegresiÃ³n LogÃ­stica - Accuracy: 0.974")
print(f"â€¢ Redes Neuronales - Accuracy: 0.956")
print(f"â€¢ RegresiÃ³n Ridge - Accuracy: 0.956")
print(f"â€¢ SVM RBF - Accuracy: {accuracy:.3f}")

# ComparaciÃ³n especÃ­fica con SVM Lineal
print(f"\nğŸ”„ SVM LINEAL vs SVM RBF:")
print(f"â€¢ Accuracy: 0.982 vs {accuracy:.3f} ({'ğŸ† RBF mejor' if accuracy > 0.982 else 'ğŸ† Lineal mejor' if accuracy < 0.982 else 'ğŸ¤ Empate'})")
print(f"â€¢ Vectores de soporte: 51 (11.2%) vs {np.sum(best_svm_rbf.n_support_)} ({np.sum(best_svm_rbf.n_support_)/len(X_train)*100:.1f}%)")
print(f"â€¢ Tiempo: 0.0065s vs {training_time:.4f}s")

# =============================================================================
# 9. RESUMEN FINAL PARA EL PAPER
# =============================================================================

print(f"\n" + "="*60)
print("RESUMEN PARA EL PAPER - SVM RBF")
print("="*60)

print(f"\nğŸ“Š RESULTADOS FINALES:")
print(f"â€¢ HiperparÃ¡metros Ã³ptimos:")
print(f"  - C: {svm_rbf_grid.best_params_['C']}")
print(f"  - Gamma: {svm_rbf_grid.best_params_['gamma']}")
print(f"â€¢ PrecisiÃ³n (Accuracy): {accuracy:.3f}")
print(f"â€¢ PrecisiÃ³n (Precision): {precision:.3f}")
print(f"â€¢ Sensibilidad (Recall): {recall:.3f}")
print(f"â€¢ PuntuaciÃ³n F1: {f1:.3f}")
print(f"â€¢ AUC-ROC: {auc_roc:.3f}")
print(f"â€¢ Tiempo de Convergencia: {training_time:.4f}s")
print(f"â€¢ Vectores de Soporte: {np.sum(best_svm_rbf.n_support_)} ({np.sum(best_svm_rbf.n_support_)/len(X_train)*100:.1f}%)")

print(f"\nğŸ¯ INTERPRETACIÃ“N CLÃNICA:")
if recall >= 0.95:
    print("â€¢ Excelente detecciÃ³n de casos malignos (recall alto)")
elif recall >= 0.90:
    print("â€¢ Buena detecciÃ³n de casos malignos")
else:
    print("â€¢ DetecciÃ³n moderada de casos malignos")

if precision >= 0.95:
    print("â€¢ Muy pocos falsos positivos (precision alta)")
elif precision >= 0.90:
    print("â€¢ Pocos falsos positivos")
else:
    print("â€¢ Algunos falsos positivos presentes")

print(f"\nâœ¨ VENTAJAS DE SVM RBF:")
print("â€¢ Capacidad de modelar fronteras de decisiÃ³n no lineales")
print("â€¢ Mapeo implÃ­cito a espacio de alta dimensiÃ³n")
print("â€¢ Robusto ante valores atÃ­picos")
print("â€¢ Flexible con parÃ¡metro gamma")

print(f"\nâš ï¸ CONSIDERACIONES NO CONVEXAS:")
print("â€¢ OptimizaciÃ³n no convexa debido al kernel no lineal")
print("â€¢ Sensible a la selecciÃ³n de hiperparÃ¡metros C y gamma")
print("â€¢ Mayor complejidad computacional que SVM lineal")
print("â€¢ Riesgo de sobreajuste con gamma alto")

print(f"\nğŸ”§ COMPLEJIDAD DEL MODELO:")
complexity_ratio = np.sum(best_svm_rbf.n_support_) / 51  # Comparado con SVM lineal
print(f"â€¢ Complejidad relativa: {complexity_ratio:.1f}x vs SVM Lineal")
print(f"â€¢ Tiempo relativo: {training_time/0.0065:.1f}x vs SVM Lineal")

print(f"\nğŸ­ COMPARACIÃ“N KERNEL LINEAL vs RBF:")
if accuracy > 0.982:
    print("â€¢ El kernel RBF mejorÃ³ el rendimiento - dataset beneficia de no linealidad")
elif accuracy == 0.982:
    print("â€¢ Rendimiento equivalente - dataset es linealmente separable")
else:
    print("â€¢ El kernel lineal fue mejor - dataset no requiere modelado no lineal")

print(f"\n" + "="*60)
print("Â¡ImplementaciÃ³n de SVM RBF completada!")
print("="*60)