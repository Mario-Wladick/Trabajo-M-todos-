# Implementaci√≥n de Regresi√≥n Ridge para Dataset de C√°ncer de Mama Wisconsin
# Proyecto: T√©cnicas de Optimizaci√≥n Convexa y No Convexa

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix, mean_squared_error, r2_score
import time
import matplotlib.pyplot as plt
import seaborn as sns

# =============================================================================
# 1. CARGA Y EXPLORACI√ìN DEL DATASET
# =============================================================================

print("="*60)
print("IMPLEMENTACI√ìN DE REGRESI√ìN RIDGE")
print("Dataset: Wisconsin Breast Cancer")
print("="*60)

# Cargar el dataset
data = load_breast_cancer()
X = data.data  # Caracter√≠sticas (30 features)
y = data.target  # Etiquetas (0=maligno, 1=benigno)

print(f"\nüìä INFORMACI√ìN DEL DATASET:")
print(f"‚Ä¢ N√∫mero de muestras: {X.shape[0]}")
print(f"‚Ä¢ N√∫mero de caracter√≠sticas: {X.shape[1]}")
print(f"‚Ä¢ Clases originales: {data.target_names}")
print(f"‚Ä¢ Para Ridge: Convertimos a problema de regresi√≥n")
print(f"‚Ä¢ Valores objetivo: 0 (maligno) ‚Üí 1 (benigno)")

# =============================================================================
# 2. PREPROCESAMIENTO DE DATOS
# =============================================================================

print(f"\nüîß PREPROCESAMIENTO:")

# Divisi√≥n entrenamiento/prueba (80/20) - MISMA DIVISI√ìN QUE ANTERIORES
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"‚Ä¢ Conjunto de entrenamiento: {X_train.shape[0]} muestras")
print(f"‚Ä¢ Conjunto de prueba: {X_test.shape[0]} muestras")

# Estandarizaci√≥n
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"‚Ä¢ Estandarizaci√≥n aplicada ‚úì")
print(f"‚Ä¢ Media de caracter√≠sticas despu√©s de escalar: {np.mean(X_train_scaled, axis=0)[:3].round(3)}")
print(f"‚Ä¢ Desviaci√≥n est√°ndar despu√©s de escalar: {np.std(X_train_scaled, axis=0)[:3].round(3)}")

# =============================================================================
# 3. OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS
# =============================================================================

print(f"\n‚öôÔ∏è OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS:")

# Definir grid de b√∫squeda para el par√°metro de regularizaci√≥n
param_grid = {
    'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
}

# Para Ridge usaremos R¬≤ como m√©trica de optimizaci√≥n, luego convertiremos a clasificaci√≥n
ridge_grid = GridSearchCV(
    Ridge(random_state=42),
    param_grid,
    cv=5,
    scoring='r2',  # R¬≤ para regresi√≥n
    n_jobs=-1,
    verbose=0
)

print("‚Ä¢ Ejecutando Grid Search con validaci√≥n cruzada 5-fold...")
start_time = time.time()
ridge_grid.fit(X_train_scaled, y_train)
grid_time = time.time() - start_time

print(f"‚Ä¢ Mejores hiperpar√°metros: {ridge_grid.best_params_}")
print(f"‚Ä¢ Mejor puntuaci√≥n R¬≤ (CV): {ridge_grid.best_score_:.4f}")
print(f"‚Ä¢ Tiempo de optimizaci√≥n: {grid_time:.2f} segundos")

# =============================================================================
# 4. ENTRENAMIENTO DEL MODELO FINAL
# =============================================================================

print(f"\nüöÄ ENTRENAMIENTO DEL MODELO FINAL:")

# Usar los mejores hiperpar√°metros
best_ridge = ridge_grid.best_estimator_

# Medir tiempo de convergencia
start_time = time.time()
best_ridge.fit(X_train_scaled, y_train)
training_time = time.time() - start_time

print(f"‚Ä¢ Modelo Ridge entrenado con hiperpar√°metros √≥ptimos")
print(f"‚Ä¢ Tiempo de convergencia: {training_time:.4f} segundos")
print(f"‚Ä¢ Coeficiente de determinaci√≥n R¬≤: {best_ridge.score(X_train_scaled, y_train):.4f}")

# =============================================================================
# 5. PREDICCIONES Y CONVERSI√ìN A CLASIFICACI√ìN
# =============================================================================

print(f"\nüîÑ CONVERSI√ìN DE REGRESI√ìN A CLASIFICACI√ìN:")

# Predicciones continuas
y_pred_continuous = best_ridge.predict(X_test_scaled)
print(f"‚Ä¢ Predicciones continuas - Rango: [{y_pred_continuous.min():.3f}, {y_pred_continuous.max():.3f}]")
print(f"‚Ä¢ Media de predicciones: {y_pred_continuous.mean():.3f}")

# Convertir a clasificaci√≥n binaria usando umbral 0.5
threshold = 0.5
y_pred = (y_pred_continuous >= threshold).astype(int)

print(f"‚Ä¢ Umbral de clasificaci√≥n: {threshold}")
print(f"‚Ä¢ Conversi√≥n: valores ‚â• {threshold} ‚Üí clase 1 (benigno)")
print(f"‚Ä¢ Conversi√≥n: valores < {threshold} ‚Üí clase 0 (maligno)")

# =============================================================================
# 6. EVALUACI√ìN DEL MODELO
# =============================================================================

print(f"\nüìä EVALUACI√ìN DEL MODELO:")

# Calcular m√©tricas de regresi√≥n
mse = mean_squared_error(y_test, y_pred_continuous)
r2 = r2_score(y_test, y_pred_continuous)

print(f"\nüìà M√âTRICAS DE REGRESI√ìN:")
print(f"‚Ä¢ Error Cuadr√°tico Medio (MSE): {mse:.4f}")
print(f"‚Ä¢ Coeficiente de Determinaci√≥n (R¬≤): {r2:.4f}")

# Calcular m√©tricas de clasificaci√≥n
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Para AUC-ROC usamos las predicciones continuas como probabilidades
# Normalizar las predicciones continuas a [0,1] para simular probabilidades
y_pred_proba_normalized = (y_pred_continuous - y_pred_continuous.min()) / (y_pred_continuous.max() - y_pred_continuous.min())
auc_roc = roc_auc_score(y_test, y_pred_proba_normalized)

print(f"\nüìà M√âTRICAS DE CLASIFICACI√ìN:")
print(f"‚Ä¢ Precisi√≥n (Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"‚Ä¢ Precisi√≥n (Precision): {precision:.4f} ({precision*100:.2f}%)")
print(f"‚Ä¢ Sensibilidad (Recall): {recall:.4f} ({recall*100:.2f}%)")
print(f"‚Ä¢ Puntuaci√≥n F1: {f1:.4f} ({f1*100:.2f}%)")
print(f"‚Ä¢ AUC-ROC: {auc_roc:.4f} ({auc_roc*100:.2f}%)")
print(f"‚Ä¢ Tiempo de Convergencia: {training_time:.4f} segundos")

# =============================================================================
# 7. AN√ÅLISIS DETALLADO
# =============================================================================

print(f"\nüîç AN√ÅLISIS DETALLADO:")

# Matriz de confusi√≥n
cm = confusion_matrix(y_test, y_pred)
print(f"\nüìã MATRIZ DE CONFUSI√ìN:")
print(f"                Predicho")
print(f"              Maligno  Benigno")
print(f"Real Maligno     {cm[0,0]:3d}     {cm[0,1]:3d}")
print(f"     Benigno     {cm[1,0]:3d}     {cm[1,1]:3d}")

# Calcular falsos positivos y negativos
tn, fp, fn, tp = cm.ravel()
print(f"\nüìä DESGLOSE DE PREDICCIONES:")
print(f"‚Ä¢ Verdaderos Positivos (TP): {tp}")
print(f"‚Ä¢ Verdaderos Negativos (TN): {tn}")
print(f"‚Ä¢ Falsos Positivos (FP): {fp}")
print(f"‚Ä¢ Falsos Negativos (FN): {fn}")

# An√°lisis de la distribuci√≥n de predicciones
print(f"\nüìä DISTRIBUCI√ìN DE PREDICCIONES CONTINUAS:")
print(f"‚Ä¢ Predicciones para clase 0 (maligno):")
malignant_predictions = y_pred_continuous[y_test == 0]
print(f"  - Media: {malignant_predictions.mean():.3f}")
print(f"  - Desv. Est√°ndar: {malignant_predictions.std():.3f}")
print(f"  - Rango: [{malignant_predictions.min():.3f}, {malignant_predictions.max():.3f}]")

print(f"‚Ä¢ Predicciones para clase 1 (benigno):")
benign_predictions = y_pred_continuous[y_test == 1]
print(f"  - Media: {benign_predictions.mean():.3f}")
print(f"  - Desv. Est√°ndar: {benign_predictions.std():.3f}")
print(f"  - Rango: [{benign_predictions.min():.3f}, {benign_predictions.max():.3f}]")

# Caracter√≠sticas m√°s importantes
feature_importance = np.abs(best_ridge.coef_)
feature_names = data.feature_names
top_features_idx = np.argsort(feature_importance)[-10:][::-1]

print(f"\nüîù TOP 10 CARACTER√çSTICAS M√ÅS IMPORTANTES:")
for i, idx in enumerate(top_features_idx):
    print(f"  {i+1:2d}. {feature_names[idx]:<25} | Coeficiente: {best_ridge.coef_[idx]:7.3f}")

# =============================================================================
# 8. VALIDACI√ìN CRUZADA ADICIONAL
# =============================================================================

print(f"\n‚úÖ VALIDACI√ìN CRUZADA FINAL:")

# Para validaci√≥n cruzada, necesitamos crear una funci√≥n que convierta regresi√≥n a clasificaci√≥n
def ridge_classification_score(estimator, X, y):
    y_pred_cont = estimator.predict(X)
    y_pred_class = (y_pred_cont >= 0.5).astype(int)
    return f1_score(y, y_pred_class)

# Validaci√≥n cruzada con conversi√≥n a clasificaci√≥n
cv_scores = cross_val_score(best_ridge, X_train_scaled, y_train, cv=5, scoring=ridge_classification_score)
cv_r2 = cross_val_score(best_ridge, X_train_scaled, y_train, cv=5, scoring='r2')

print(f"‚Ä¢ F1-Score CV (5-fold): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
print(f"‚Ä¢ R¬≤ CV (5-fold): {cv_r2.mean():.4f} ¬± {cv_r2.std():.4f}")

# =============================================================================
# 9. COMPARACI√ìN CON M√âTODOS ANTERIORES
# =============================================================================

print(f"\nüÜö COMPARACI√ìN R√ÅPIDA:")
print("(Valores de referencia de m√©todos anteriores)")
print(f"‚Ä¢ Regresi√≥n Log√≠stica - Accuracy: 0.974")
print(f"‚Ä¢ SVM Lineal - Accuracy: 0.982")
print(f"‚Ä¢ Regresi√≥n Ridge - Accuracy: {accuracy:.3f}")

# =============================================================================
# 10. RESUMEN FINAL PARA EL PAPER
# =============================================================================

print(f"\n" + "="*60)
print("RESUMEN PARA EL PAPER - REGRESI√ìN RIDGE")
print("="*60)

print(f"\nüìä RESULTADOS FINALES:")
print(f"‚Ä¢ Hiperpar√°metros √≥ptimos: alpha={ridge_grid.best_params_['alpha']}")
print(f"‚Ä¢ Precisi√≥n (Accuracy): {accuracy:.3f}")
print(f"‚Ä¢ Precisi√≥n (Precision): {precision:.3f}")
print(f"‚Ä¢ Sensibilidad (Recall): {recall:.3f}")
print(f"‚Ä¢ Puntuaci√≥n F1: {f1:.3f}")
print(f"‚Ä¢ AUC-ROC: {auc_roc:.3f}")
print(f"‚Ä¢ Tiempo de Convergencia: {training_time:.4f}s")
print(f"‚Ä¢ R¬≤ (como regresi√≥n): {r2:.3f}")

print(f"\nüéØ INTERPRETACI√ìN CL√çNICA:")
if recall >= 0.95:
    print("‚Ä¢ Excelente detecci√≥n de casos malignos (recall alto)")
elif recall >= 0.90:
    print("‚Ä¢ Buena detecci√≥n de casos malignos")
else:
    print("‚Ä¢ Detecci√≥n moderada de casos malignos")

if precision >= 0.95:
    print("‚Ä¢ Muy pocos falsos positivos (precision alta)")
elif precision >= 0.90:
    print("‚Ä¢ Pocos falsos positivos")
else:
    print("‚Ä¢ Algunos falsos positivos presentes")

print(f"\n‚ú® VENTAJAS DE REGRESI√ìN RIDGE:")
print("‚Ä¢ Previene sobreajuste con regularizaci√≥n L2")
print("‚Ä¢ Maneja bien la multicolinealidad")
print("‚Ä¢ Soluci√≥n anal√≠tica directa (muy r√°pida)")
print("‚Ä¢ Estable num√©ricamente")

print(f"\nüìä CONSIDERACIONES ESPECIALES:")
print("‚Ä¢ Adaptada de regresi√≥n a clasificaci√≥n")
print("‚Ä¢ Umbral de 0.5 para conversi√≥n binaria")
print("‚Ä¢ Predicciones continuas proporcionan informaci√≥n adicional")

print(f"\nüîÑ SEPARACI√ìN DE CLASES:")
print(f"‚Ä¢ Predicciones promedio para malignos: {malignant_predictions.mean():.3f}")
print(f"‚Ä¢ Predicciones promedio para benignos: {benign_predictions.mean():.3f}")
print(f"‚Ä¢ Separaci√≥n: {abs(benign_predictions.mean() - malignant_predictions.mean()):.3f}")

print(f"\n" + "="*60)
print("¬°Implementaci√≥n de Regresi√≥n Ridge completada!")
print("="*60)