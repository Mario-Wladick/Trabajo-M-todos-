# Implementaci√≥n de SVM Lineal para Dataset de C√°ncer de Mama Wisconsin
# Proyecto: T√©cnicas de Optimizaci√≥n Convexa y No Convexa

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix
import time
import matplotlib.pyplot as plt
import seaborn as sns

# =============================================================================
# 1. CARGA Y EXPLORACI√ìN DEL DATASET
# =============================================================================

print("="*60)
print("IMPLEMENTACI√ìN DE SVM LINEAL")
print("Dataset: Wisconsin Breast Cancer")
print("="*60)

# Cargar el dataset
data = load_breast_cancer()
X = data.data  # Caracter√≠sticas (30 features)
y = data.target  # Etiquetas (0=maligno, 1=benigno)

print(f"\nüìä INFORMACI√ìN DEL DATASET:")
print(f"‚Ä¢ N√∫mero de muestras: {X.shape[0]}")
print(f"‚Ä¢ N√∫mero de caracter√≠sticas: {X.shape[1]}")
print(f"‚Ä¢ Clases: {data.target_names}")
print(f"‚Ä¢ Distribuci√≥n de clases:")
unique, counts = np.unique(y, return_counts=True)
for i, (clase, count) in enumerate(zip(data.target_names, counts)):
    print(f"  - {clase}: {count} ({count/len(y)*100:.1f}%)")

# =============================================================================
# 2. PREPROCESAMIENTO DE DATOS
# =============================================================================

print(f"\nüîß PREPROCESAMIENTO:")

# Divisi√≥n entrenamiento/prueba (80/20) - MISMA DIVISI√ìN QUE REGRESI√ìN LOG√çSTICA
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"‚Ä¢ Conjunto de entrenamiento: {X_train.shape[0]} muestras")
print(f"‚Ä¢ Conjunto de prueba: {X_test.shape[0]} muestras")

# Estandarizaci√≥n
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"‚Ä¢ Estandarizaci√≥n aplicada ‚úì")
print(f"‚Ä¢ Media de caracter√≠sticas despu√©s de escalar: {np.mean(X_train_scaled, axis=0)[:3].round(3)}")
print(f"‚Ä¢ Desviaci√≥n est√°ndar despu√©s de escalar: {np.std(X_train_scaled, axis=0)[:3].round(3)}")

# =============================================================================
# 3. OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS
# =============================================================================

print(f"\n‚öôÔ∏è OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS:")

# Definir grid de b√∫squeda para el par√°metro de regularizaci√≥n
param_grid = {
    'C': [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],
    'gamma': ['scale', 'auto']  # Para kernel lineal no es tan relevante, pero lo incluimos
}

# Grid Search con validaci√≥n cruzada
svm_grid = GridSearchCV(
    SVC(kernel='linear', random_state=42, probability=True),
    param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1,
    verbose=0
)

print("‚Ä¢ Ejecutando Grid Search con validaci√≥n cruzada 5-fold...")
start_time = time.time()
svm_grid.fit(X_train_scaled, y_train)
grid_time = time.time() - start_time

print(f"‚Ä¢ Mejores hiperpar√°metros: {svm_grid.best_params_}")
print(f"‚Ä¢ Mejor puntuaci√≥n F1 (CV): {svm_grid.best_score_:.4f}")
print(f"‚Ä¢ Tiempo de optimizaci√≥n: {grid_time:.2f} segundos")

# =============================================================================
# 4. ENTRENAMIENTO DEL MODELO FINAL
# =============================================================================

print(f"\nüöÄ ENTRENAMIENTO DEL MODELO FINAL:")

# Usar los mejores hiperpar√°metros
best_svm = svm_grid.best_estimator_

# Medir tiempo de convergencia
start_time = time.time()
best_svm.fit(X_train_scaled, y_train)
training_time = time.time() - start_time

print(f"‚Ä¢ Modelo SVM Lineal entrenado con hiperpar√°metros √≥ptimos")
print(f"‚Ä¢ Tiempo de convergencia: {training_time:.4f} segundos")
print(f"‚Ä¢ N√∫mero de vectores de soporte: {best_svm.n_support_}")
print(f"‚Ä¢ Total de vectores de soporte: {np.sum(best_svm.n_support_)} de {len(X_train)} muestras ({np.sum(best_svm.n_support_)/len(X_train)*100:.1f}%)")

# =============================================================================
# 5. EVALUACI√ìN DEL MODELO
# =============================================================================

print(f"\nüìä EVALUACI√ìN DEL MODELO:")

# Predicciones
y_pred = best_svm.predict(X_test_scaled)
y_pred_proba = best_svm.predict_proba(X_test_scaled)[:, 1]

# Calcular m√©tricas
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc_roc = roc_auc_score(y_test, y_pred_proba)

# Mostrar resultados
print(f"\nüìà M√âTRICAS DE RENDIMIENTO:")
print(f"‚Ä¢ Precisi√≥n (Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"‚Ä¢ Precisi√≥n (Precision): {precision:.4f} ({precision*100:.2f}%)")
print(f"‚Ä¢ Sensibilidad (Recall): {recall:.4f} ({recall*100:.2f}%)")
print(f"‚Ä¢ Puntuaci√≥n F1: {f1:.4f} ({f1*100:.2f}%)")
print(f"‚Ä¢ AUC-ROC: {auc_roc:.4f} ({auc_roc*100:.2f}%)")
print(f"‚Ä¢ Tiempo de Convergencia: {training_time:.4f} segundos")

# =============================================================================
# 6. AN√ÅLISIS DETALLADO
# =============================================================================

print(f"\nüîç AN√ÅLISIS DETALLADO:")

# Matriz de confusi√≥n
cm = confusion_matrix(y_test, y_pred)
print(f"\nüìã MATRIZ DE CONFUSI√ìN:")
print(f"                Predicho")
print(f"              Maligno  Benigno")
print(f"Real Maligno     {cm[0,0]:3d}     {cm[0,1]:3d}")
print(f"     Benigno     {cm[1,0]:3d}     {cm[1,1]:3d}")

# Calcular falsos positivos y negativos
tn, fp, fn, tp = cm.ravel()
print(f"\nüìä DESGLOSE DE PREDICCIONES:")
print(f"‚Ä¢ Verdaderos Positivos (TP): {tp}")
print(f"‚Ä¢ Verdaderos Negativos (TN): {tn}")
print(f"‚Ä¢ Falsos Positivos (FP): {fp}")
print(f"‚Ä¢ Falsos Negativos (FN): {fn}")

# An√°lisis de vectores de soporte
print(f"\nüéØ AN√ÅLISIS DE VECTORES DE SOPORTE:")
print(f"‚Ä¢ Vectores de soporte por clase: {best_svm.n_support_}")
print(f"‚Ä¢ Porcentaje de muestras que son vectores de soporte: {np.sum(best_svm.n_support_)/len(X_train)*100:.1f}%")
print(f"‚Ä¢ Esto indica qu√© tan compleja es la frontera de decisi√≥n")

# Coeficientes del hiperplano (para kernel lineal)
if hasattr(best_svm, 'coef_'):
    feature_importance = np.abs(best_svm.coef_[0])
    feature_names = data.feature_names
    top_features_idx = np.argsort(feature_importance)[-10:][::-1]
    
    print(f"\nüîù TOP 10 CARACTER√çSTICAS M√ÅS IMPORTANTES:")
    for i, idx in enumerate(top_features_idx):
        print(f"  {i+1:2d}. {feature_names[idx]:<25} | Coeficiente: {best_svm.coef_[0][idx]:7.3f}")

# =============================================================================
# 7. VALIDACI√ìN CRUZADA ADICIONAL
# =============================================================================

print(f"\n‚úÖ VALIDACI√ìN CRUZADA FINAL:")

# Validaci√≥n cruzada con m√∫ltiples m√©tricas
cv_accuracy = cross_val_score(best_svm, X_train_scaled, y_train, cv=5, scoring='accuracy')
cv_precision = cross_val_score(best_svm, X_train_scaled, y_train, cv=5, scoring='precision')
cv_recall = cross_val_score(best_svm, X_train_scaled, y_train, cv=5, scoring='recall')
cv_f1 = cross_val_score(best_svm, X_train_scaled, y_train, cv=5, scoring='f1')

print(f"‚Ä¢ Accuracy CV (5-fold): {cv_accuracy.mean():.4f} ¬± {cv_accuracy.std():.4f}")
print(f"‚Ä¢ Precision CV (5-fold): {cv_precision.mean():.4f} ¬± {cv_precision.std():.4f}")
print(f"‚Ä¢ Recall CV (5-fold): {cv_recall.mean():.4f} ¬± {cv_recall.std():.4f}")
print(f"‚Ä¢ F1-Score CV (5-fold): {cv_f1.mean():.4f} ¬± {cv_f1.std():.4f}")

# =============================================================================
# 8. COMPARACI√ìN CON REGRESI√ìN LOG√çSTICA
# =============================================================================

print(f"\nüÜö COMPARACI√ìN R√ÅPIDA:")
print("(Valores de referencia de Regresi√≥n Log√≠stica)")
print(f"‚Ä¢ Regresi√≥n Log√≠stica - Accuracy: 0.974")
print(f"‚Ä¢ SVM Lineal - Accuracy: {accuracy:.3f}")
print(f"‚Ä¢ Diferencia: {accuracy - 0.974:+.3f}")

# =============================================================================
# 9. RESUMEN FINAL PARA EL PAPER
# =============================================================================

print(f"\n" + "="*60)
print("RESUMEN PARA EL PAPER - SVM LINEAL")
print("="*60)

print(f"\nüìä RESULTADOS FINALES:")
print(f"‚Ä¢ Hiperpar√°metros √≥ptimos: C={svm_grid.best_params_['C']}, gamma='{svm_grid.best_params_['gamma']}'")
print(f"‚Ä¢ Precisi√≥n (Accuracy): {accuracy:.3f}")
print(f"‚Ä¢ Precisi√≥n (Precision): {precision:.3f}")
print(f"‚Ä¢ Sensibilidad (Recall): {recall:.3f}")
print(f"‚Ä¢ Puntuaci√≥n F1: {f1:.3f}")
print(f"‚Ä¢ AUC-ROC: {auc_roc:.3f}")
print(f"‚Ä¢ Tiempo de Convergencia: {training_time:.4f}s")
print(f"‚Ä¢ Vectores de Soporte: {np.sum(best_svm.n_support_)} ({np.sum(best_svm.n_support_)/len(X_train)*100:.1f}%)")

print(f"\nüéØ INTERPRETACI√ìN CL√çNICA:")
if recall >= 0.95:
    print("‚Ä¢ Excelente detecci√≥n de casos malignos (recall alto)")
elif recall >= 0.90:
    print("‚Ä¢ Buena detecci√≥n de casos malignos")
else:
    print("‚Ä¢ Detecci√≥n moderada de casos malignos")

if precision >= 0.95:
    print("‚Ä¢ Muy pocos falsos positivos (precision alta)")
elif precision >= 0.90:
    print("‚Ä¢ Pocos falsos positivos")
else:
    print("‚Ä¢ Algunos falsos positivos presentes")

print(f"\n‚ú® VENTAJAS DE SVM LINEAL:")
print("‚Ä¢ Frontera de decisi√≥n √≥ptima (maximiza margen)")
print("‚Ä¢ Robusto ante valores at√≠picos")
print("‚Ä¢ Eficiente en memoria (solo almacena vectores de soporte)")
print("‚Ä¢ Base te√≥rica s√≥lida")

print(f"\n‚ö° EFICIENCIA:")
if np.sum(best_svm.n_support_)/len(X_train) < 0.3:
    print("‚Ä¢ Modelo eficiente: pocos vectores de soporte necesarios")
else:
    print("‚Ä¢ Modelo complejo: muchos vectores de soporte necesarios")

print(f"\n" + "="*60)
print("¬°Implementaci√≥n de SVM Lineal completada!")
print("="*60)