# Implementaci√≥n de Regresi√≥n Log√≠stica para Dataset de C√°ncer de Mama Wisconsin
# Proyecto: T√©cnicas de Optimizaci√≥n Convexa y No Convexa

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix
import time
import matplotlib.pyplot as plt
import seaborn as sns

# =============================================================================
# 1. CARGA Y EXPLORACI√ìN DEL DATASET
# =============================================================================

print("="*60)
print("IMPLEMENTACI√ìN DE REGRESI√ìN LOG√çSTICA")
print("Dataset: Wisconsin Breast Cancer")
print("="*60)

# Cargar el dataset
data = load_breast_cancer()
X = data.data  # Caracter√≠sticas (30 features)
y = data.target  # Etiquetas (0=maligno, 1=benigno)

print(f"\nüìä INFORMACI√ìN DEL DATASET:")
print(f"‚Ä¢ N√∫mero de muestras: {X.shape[0]}")
print(f"‚Ä¢ N√∫mero de caracter√≠sticas: {X.shape[1]}")
print(f"‚Ä¢ Clases: {data.target_names}")
print(f"‚Ä¢ Distribuci√≥n de clases:")
unique, counts = np.unique(y, return_counts=True)
for i, (clase, count) in enumerate(zip(data.target_names, counts)):
    print(f"  - {clase}: {count} ({count/len(y)*100:.1f}%)")

# Mostrar nombres de las caracter√≠sticas
print(f"\nüìã CARACTER√çSTICAS (primeras 10):")
for i, feature in enumerate(data.feature_names[:10]):
    print(f"  {i+1:2d}. {feature}")
print(f"  ... y {len(data.feature_names)-10} m√°s")

# =============================================================================
# 2. PREPROCESAMIENTO DE DATOS
# =============================================================================

print(f"\nüîß PREPROCESAMIENTO:")

# Divisi√≥n entrenamiento/prueba (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"‚Ä¢ Conjunto de entrenamiento: {X_train.shape[0]} muestras")
print(f"‚Ä¢ Conjunto de prueba: {X_test.shape[0]} muestras")

# Estandarizaci√≥n
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"‚Ä¢ Estandarizaci√≥n aplicada ‚úì")
print(f"‚Ä¢ Media de caracter√≠sticas despu√©s de escalar: {np.mean(X_train_scaled, axis=0)[:3].round(3)}")
print(f"‚Ä¢ Desviaci√≥n est√°ndar despu√©s de escalar: {np.std(X_train_scaled, axis=0)[:3].round(3)}")

# =============================================================================
# 3. OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS
# =============================================================================

print(f"\n‚öôÔ∏è OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS:")

# Definir grid de b√∫squeda para el par√°metro de regularizaci√≥n
param_grid = {
    'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],
    'solver': ['liblinear', 'lbfgs']
}

# Grid Search con validaci√≥n cruzada
lr_grid = GridSearchCV(
    LogisticRegression(random_state=42, max_iter=1000),
    param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1,
    verbose=0
)

print("‚Ä¢ Ejecutando Grid Search con validaci√≥n cruzada 5-fold...")
start_time = time.time()
lr_grid.fit(X_train_scaled, y_train)
grid_time = time.time() - start_time

print(f"‚Ä¢ Mejores hiperpar√°metros: {lr_grid.best_params_}")
print(f"‚Ä¢ Mejor puntuaci√≥n F1 (CV): {lr_grid.best_score_:.4f}")
print(f"‚Ä¢ Tiempo de optimizaci√≥n: {grid_time:.2f} segundos")

# =============================================================================
# 4. ENTRENAMIENTO DEL MODELO FINAL
# =============================================================================

print(f"\nüöÄ ENTRENAMIENTO DEL MODELO FINAL:")

# Usar los mejores hiperpar√°metros
best_lr = lr_grid.best_estimator_

# Medir tiempo de convergencia
start_time = time.time()
best_lr.fit(X_train_scaled, y_train)
training_time = time.time() - start_time

print(f"‚Ä¢ Modelo entrenado con hiperpar√°metros √≥ptimos")
print(f"‚Ä¢ Tiempo de convergencia: {training_time:.4f} segundos")
print(f"‚Ä¢ N√∫mero de iteraciones hasta convergencia: {best_lr.n_iter_[0]}")

# =============================================================================
# 5. EVALUACI√ìN DEL MODELO
# =============================================================================

print(f"\nüìä EVALUACI√ìN DEL MODELO:")

# Predicciones
y_pred = best_lr.predict(X_test_scaled)
y_pred_proba = best_lr.predict_proba(X_test_scaled)[:, 1]

# Calcular m√©tricas
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc_roc = roc_auc_score(y_test, y_pred_proba)

# Mostrar resultados
print(f"\nüìà M√âTRICAS DE RENDIMIENTO:")
print(f"‚Ä¢ Precisi√≥n (Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"‚Ä¢ Precisi√≥n (Precision): {precision:.4f} ({precision*100:.2f}%)")
print(f"‚Ä¢ Sensibilidad (Recall): {recall:.4f} ({recall*100:.2f}%)")
print(f"‚Ä¢ Puntuaci√≥n F1: {f1:.4f} ({f1*100:.2f}%)")
print(f"‚Ä¢ AUC-ROC: {auc_roc:.4f} ({auc_roc*100:.2f}%)")
print(f"‚Ä¢ Tiempo de Convergencia: {training_time:.4f} segundos")

# =============================================================================
# 6. AN√ÅLISIS DETALLADO
# =============================================================================

print(f"\nüîç AN√ÅLISIS DETALLADO:")

# Matriz de confusi√≥n
cm = confusion_matrix(y_test, y_pred)
print(f"\nüìã MATRIZ DE CONFUSI√ìN:")
print(f"                Predicho")
print(f"              Maligno  Benigno")
print(f"Real Maligno     {cm[0,0]:3d}     {cm[0,1]:3d}")
print(f"     Benigno     {cm[1,0]:3d}     {cm[1,1]:3d}")

# Calcular falsos positivos y negativos
tn, fp, fn, tp = cm.ravel()
print(f"\nüìä DESGLOSE DE PREDICCIONES:")
print(f"‚Ä¢ Verdaderos Positivos (TP): {tp}")
print(f"‚Ä¢ Verdaderos Negativos (TN): {tn}")
print(f"‚Ä¢ Falsos Positivos (FP): {fp}")
print(f"‚Ä¢ Falsos Negativos (FN): {fn}")

# Caracter√≠sticas m√°s importantes
feature_importance = np.abs(best_lr.coef_[0])
feature_names = data.feature_names
top_features_idx = np.argsort(feature_importance)[-10:][::-1]

print(f"\nüîù TOP 10 CARACTER√çSTICAS M√ÅS IMPORTANTES:")
for i, idx in enumerate(top_features_idx):
    print(f"  {i+1:2d}. {feature_names[idx]:<25} | Coeficiente: {best_lr.coef_[0][idx]:7.3f}")

# =============================================================================
# 7. VALIDACI√ìN CRUZADA ADICIONAL
# =============================================================================

print(f"\n‚úÖ VALIDACI√ìN CRUZADA FINAL:")

# Validaci√≥n cruzada con m√∫ltiples m√©tricas
cv_accuracy = cross_val_score(best_lr, X_train_scaled, y_train, cv=5, scoring='accuracy')
cv_precision = cross_val_score(best_lr, X_train_scaled, y_train, cv=5, scoring='precision')
cv_recall = cross_val_score(best_lr, X_train_scaled, y_train, cv=5, scoring='recall')
cv_f1 = cross_val_score(best_lr, X_train_scaled, y_train, cv=5, scoring='f1')

print(f"‚Ä¢ Accuracy CV (5-fold): {cv_accuracy.mean():.4f} ¬± {cv_accuracy.std():.4f}")
print(f"‚Ä¢ Precision CV (5-fold): {cv_precision.mean():.4f} ¬± {cv_precision.std():.4f}")
print(f"‚Ä¢ Recall CV (5-fold): {cv_recall.mean():.4f} ¬± {cv_recall.std():.4f}")
print(f"‚Ä¢ F1-Score CV (5-fold): {cv_f1.mean():.4f} ¬± {cv_f1.std():.4f}")

# =============================================================================
# 8. RESUMEN FINAL PARA EL PAPER
# =============================================================================

print(f"\n" + "="*60)
print("RESUMEN PARA EL PAPER - REGRESI√ìN LOG√çSTICA")
print("="*60)

print(f"\nüìä RESULTADOS FINALES:")
print(f"‚Ä¢ Hiperpar√°metros √≥ptimos: C={lr_grid.best_params_['C']}, solver='{lr_grid.best_params_['solver']}'")
print(f"‚Ä¢ Precisi√≥n (Accuracy): {accuracy:.3f}")
print(f"‚Ä¢ Precisi√≥n (Precision): {precision:.3f}")
print(f"‚Ä¢ Sensibilidad (Recall): {recall:.3f}")
print(f"‚Ä¢ Puntuaci√≥n F1: {f1:.3f}")
print(f"‚Ä¢ AUC-ROC: {auc_roc:.3f}")
print(f"‚Ä¢ Tiempo de Convergencia: {training_time:.4f}s")

print(f"\nüéØ INTERPRETACI√ìN CL√çNICA:")
if recall >= 0.95:
    print("‚Ä¢ Excelente detecci√≥n de casos malignos (recall alto)")
elif recall >= 0.90:
    print("‚Ä¢ Buena detecci√≥n de casos malignos")
else:
    print("‚Ä¢ Detecci√≥n moderada de casos malignos")

if precision >= 0.95:
    print("‚Ä¢ Muy pocos falsos positivos (precision alta)")
elif precision >= 0.90:
    print("‚Ä¢ Pocos falsos positivos")
else:
    print("‚Ä¢ Algunos falsos positivos presentes")

print(f"\n‚ú® VENTAJAS DE REGRESI√ìN LOG√çSTICA:")
print("‚Ä¢ Convergencia r√°pida garantizada (m√©todo convexo)")
print("‚Ä¢ Resultados interpretables")
print("‚Ä¢ Probabilidades de predicci√≥n disponibles")
print("‚Ä¢ Robusto y estable")

print(f"\n" + "="*60)
print("¬°Implementaci√≥n completada exitosamente!")
print("="*60)